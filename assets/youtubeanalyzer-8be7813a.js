import{a as n,F as r,j as e,S as s,c as i,R as l,b as p,M as h,d as c,e as d}from"./faviconsetter-da5e80b9.js";import{N as u,a as m}from"./navbarphone-a27821dc.js";import{S as f,a as g}from"./scrollbar-85b404cc.js";import{S as b,B as y,a as o,b as t,c as w}from"./blogfootergap-4b4a74a4.js";import{S as x}from"./scrollablepage-a2dbc0aa.js";import{B as a}from"./blogimage-1e0f1d97.js";const v={id:"",pages:[{text:"About Me",href:"/aboutme.html"},{text:"Projects",href:"/projects.html"},{text:"Contact",href:"/contact.html"}],activepage:-1,enablepc:"true",zindexpc:"1",anchorpointpc:"topleft",sizingmodepc:"explicit",widthpc:"100",heightpc:"6",xpc:"0",ypc:"0",enablephone:"false",zindexphone:"",anchorpointphone:"",sizingmodephone:"",widthphone:"",heightphone:"",xphone:"",yphone:""},P={id:"",pages:[{text:"Home",href:"/?skipOverlay=true"},{text:"About Me",href:"/aboutme.html"},{text:"Projects",href:"/projects.html"},{text:"Contact",href:"/contact.html"}],activepage:-1,enablepc:"false",enablephone:"true",zindexphone:"100",anchorpointphone:"topleft",sizingmodephone:"explicit",widthphone:"100",heightphone:"6",xphone:"0",yphone:"0"},I={id:"",src:"/src/img/bg-stars.png",naturalwidth:"1920",naturalheight:"1080",transition:"left linear 60s",timetoscroll:"60000",enablepc:"true",zindexpc:"-2",enablephone:"true",zindexphone:"-2"},z={id:"",enablepc:"true",zindexpc:"-1",anchorpointpc:"middlemiddle",sizingmodepc:"explicit",widthpc:"70",heightpc:"105",xpc:"50",ypc:"0",srcpc:"/src/img/bg-blogwall.png",enablephone:"true",zindexphone:"-1",anchorpointphone:"middlemiddle",sizingmodephone:"explicit",widthphone:"95",heightphone:"105",xphone:"50",yphone:"0",srcphone:"/src/img/bg-blogwall.png"},S={enablepc:"true",zindexpc:"1",enablephone:"true",zindexphone:"1"},B={id:"",targetid:"scrollablebody",enablepc:"true",zindexpc:"1",scrollstrengthpc:"100",minimumanimframespc:"4",pixelsperframepc:"20",widthmodepc:"px",widthvaluepc:"10",scrollbaroffsetpc:"0",enablephone:"true",zindexphone:"1",scrollstrengthphone:"50",minimumanimframesphone:"5",pixelsperframephone:"5",widthmodephone:"px",widthvaluephone:"5",scrollbaroffsetphone:"0"},T={id:"",text:"YouTube Analysis Script",fontfamily:"",fontsize:"",fontweight:"",color:"",enablepc:"true",enablephone:"true"},k={id:"",enablepc:"true",srcpc:"/src/img/projects/youtubeanalyzer/1.png",enablephone:"true",srcphone:""},A={id:"",enablepc:"true",srcpc:"/src/img/projects/youtubeanalyzer/2.png",enablephone:"true",srcphone:""},_={id:"",text:"1. Purpose",fontfamily:"",fontsize:"",fontweight:"",color:"",enablepc:"true",enablephone:"true",whitespacephone:"nowrap"},j={id:"",text:`As a random off the cuff comment my friend said \\"I wonder how many hours of scary stories I've listened to on YouTube.\\" I didn't have a project at the time, and was curious about my own YouTube statistics because I'm an addict to an unhealthy degree. I also listen to music on the platform and was curious which song I had listened to the most. I decided to whip something together that was relatively easily made (didn't take longer than a day) and relatively easily accessible/usable (didn't require third party software installation to run). I knew one option was opting for some sort of OAuth approach but I despise that integration as I've dabbled with it in the past and it was an absolutely impossible pain attempting to run on a localhost, raw html file rather than a certified node/apache server. I also knew making all those API requests would probably end up costing money in some way. So my final decision was to do a one-time Google Takeout data request of all YouTube watch history. That saves half the API requests looking up the video ID's watched leaving only the half of looking up the video info from the ID's.`,fontfamily:"",fontsize:"",fontweight:"",color:"",enablepc:"true",enablephone:"true"},q={id:"",text:"2. Features",fontfamily:"",fontsize:"",fontweight:"",color:"",enablepc:"true",enablephone:"true",whitespacephone:"nowrap"},F={id:"",text:"- Fully detailed step-by-step instructional walkthrough of how to use application present in .zip file. Short-form instructions also present straight from .html file.<br>- General purpose as well as filterable results.<br>- Total Watchtime<br>- Most Watched Video<br>- Oldest Video On Record<br>- Random Video Watched<br>- Longest Title Video<br>- Longest Length Video",fontfamily:"",fontsize:"",fontweight:"",color:"",textalign:"left",enablepc:"true",enablephone:"true"},H={id:"",text:"3. Source/Demo",fontfamily:"",fontsize:"",fontweight:"",color:"",enablepc:"true",enablephone:"true",whitespacephone:"nowrap"},C={id:"",text:'Download: <a href=\\"/src/zip/projects/youtubeanalyzer/AaronsYoutubeAnalyzer.zip\\" style=\\"color: rgb(115,204,168)\\">Here</a>',fontfamily:"",fontsize:"",fontweight:"",color:"",textalign:"",enablepc:"true",enablephone:"true"},G={id:"",text:"4. Problems",fontfamily:"",fontsize:"",fontweight:"",color:"",enablepc:"true",enablephone:"true",whitespacephone:"nowrap"},M={id:"",text:`<p class=\\"blogtextlist\\">- Missing data. Google Takeout blatantly does not give all the data they have on file. If I go on YouTube, my watch history shows videos far exceeding what I retrieved from my Takeout request so I know they have it on file. They simply do not give it to me. Takeout also doesn't have an intentionally/consistently imposed data set limit as the number of videos included in my request was arbitary and my friends got different numbers. Google Support has had tickets about this issue spanning back over half a decade which despite being very straight forward of an issue, only got redirected and sidestepped responses at best from staff. They know this issue exists and they simply don't care to fix it.</p><br><p class=\\"blogtextlist\\">- API calls. Google only allows so many API calls for free. Their API backend also doesn't respond properly in too many ways to count for localhosted raw .html file executions. No JS interactions function. I turned to using CURL requests (still through JS but technically not using their JS library). To get information on the video ID's compiled from the Google data download, I'd run out of free API calls. So through reverse engineering by looking at the request structure of other means of call (not CURL), I was able to compile 50 videos into each request effectively making the amount of requests necessary 1/50th of what would be needed. For some reason they cap CURL request acknowledgement at 50 videos per request though.</p><br><p class=\\"blogtextlist\\">- Ease of use. This whole project was made for my friend who is not very embedded in tech. They don't have python installed so that was out. They would also probably feel suspicious executing any standalone program through Java or C. The only way to provide them comfort was to run something in browser I was sure, so I was limited to browser based (JS) functionality.</p>`,fontfamily:"",fontsize:"",fontweight:"",color:"",textalign:"",enablepc:"true",enablephone:"true"};function R(){return n(r,{children:[e(u,{...v}),e(m,{...P}),e(s,{...I}),e(f,{...z}),e(b,{}),e(x,{...S}),e(g,{...B}),e(y,{}),e(o,{...T}),e(a,{...k}),e(a,{...A}),e(o,{..._}),e(t,{...j}),e(o,{...q}),e(t,{...F}),e(o,{...H}),e(t,{...C}),e(o,{...G}),e(t,{...M}),e(w,{})]})}i.createRoot(document.getElementById("root")).render(n(l.StrictMode,{children:[e(p,{}),e(h,{}),e(c,{}),e(R,{}),e(d,{})]}));
